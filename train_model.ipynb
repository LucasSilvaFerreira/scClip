{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e8add",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Full Pipeline with Custom Combination of Peak & Frequency via a Neural Network\n",
    "# \n",
    "# Now includes:\n",
    "#   • BatchNorm on raw ViT pattern embeddings\n",
    "#   • BatchNorm on raw frequency input\n",
    "#   • BatchNorm inside the combiner MLP\n",
    "#   • Remaining LayerNorms (in projection heads and freq MLP) retained\n",
    "#   • Dropout for regularization\n",
    "#   • Optional mixed precision (torch.amp)\n",
    "#   • Extended warmup scheduler, early stopping, and checkpointing\n",
    "#\n",
    "# Requirements:\n",
    "#   • Python 3.8+\n",
    "#   • PyTorch 1.12+\n",
    "#   • vit-pytorch>=0.25.6\n",
    "#   • numpy\n",
    "#   • pandas\n",
    "#   • tqdm\n",
    "#\n",
    "# Before running:\n",
    "#   pip install torch torchvision vit-pytorch>=0.25.6 numpy pandas tqdm\n",
    "#   Ensure get_clip_encoding(chrom, start, end) is on your PYTHONPATH and returns:\n",
    "#       seq_h_out (5×32), top_out (10×100), top_amount (10,)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "# Optional mixed precision\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "# Assume get_clip_encoding is defined elsewhere and on PYTHONPATH:\n",
    "# from your_module import get_clip_encoding\n",
    "\n",
    "\n",
    "# -------------------- Flags & Hyperparameters --------------------\n",
    "LOAD_DATASET    = True      # If True, load from saved .npy files instead of generating\n",
    "SAVE_DATASET    = False     # If True (and generating), save arrays to .npy after generation\n",
    "\n",
    "n_epochs        = 10000\n",
    "batch_size      = 32 * 1\n",
    "learning_rate   = 1e-3               # you can reduce further if needed\n",
    "weight_decay    = 5e-8               # increased weight decay\n",
    "dropout_prob    = 0.3                # increased dropout\n",
    "warmup_epochs   = 50                 # longer warmup\n",
    "USE_AMP         = False              # set to True to enable mixed precision\n",
    "\n",
    "patience        = 1000                # early stopping patience\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Load promoter coordinates from BED file\n",
    "# ==============================================================================\n",
    "promoter_bed = 'promoter_view_test.bed'  # replace with actual path\n",
    "bed_df = pd.read_csv(\n",
    "    promoter_bed,\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    usecols=[0, 1, 2],\n",
    "    names=['chrom', 'start', 'end']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2.  load  intervals ±25 kb of promoter start\n",
    "# ==============================================================================\n",
    "\n",
    "seq_h_dataset      = np.load('seq_h_dataset.npy')       # shape (n_samples, 5, 32)\n",
    "top_out_dataset    = np.load('top_out_dataset.npy')     # shape (n_samples, 10, 100)\n",
    "top_amount_dataset = np.load('top_amount_dataset.npy')  # shape (n_samples, 10)\n",
    "n_samples       = seq_h_dataset.shape[0]\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Split into Training (80 %) and Validation (20 %) Sets\n",
    "# ==============================================================================\n",
    "indices = np.arange(n_samples)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split     = int(0.8 * n_samples)\n",
    "train_idx = indices[:split]\n",
    "val_idx   = indices[split:]\n",
    "\n",
    "train_seq_h      = seq_h_dataset[train_idx]\n",
    "train_top_out    = top_out_dataset[train_idx]\n",
    "train_top_amount = top_amount_dataset[train_idx]\n",
    "\n",
    "val_seq_h      = seq_h_dataset[val_idx]\n",
    "val_top_out    = top_out_dataset[val_idx]\n",
    "val_top_amount = top_amount_dataset[val_idx]\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Wrap in PyTorch Dataset and DataLoader\n",
    "# ==============================================================================\n",
    "class GenomicClipDataset(Dataset):\n",
    "    def __init__(self, seq_h_array, top_array, top_amount_array):\n",
    "        \"\"\"\n",
    "        seq_h_array:      numpy array of shape (N, 5, 32)\n",
    "        top_array:        numpy array of shape (N, 10, 100)\n",
    "        top_amount_array: numpy array of shape (N, 10)\n",
    "        \"\"\"\n",
    "        assert seq_h_array.shape[0] == top_array.shape[0] == top_amount_array.shape[0]\n",
    "        self.seq_h      = seq_h_array\n",
    "        self.top        = top_array\n",
    "        self.top_amount = top_amount_array\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.seq_h.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dna  = torch.from_numpy(self.seq_h[idx]).float()       # (5, 32)\n",
    "        pat  = torch.from_numpy(self.top[idx]).float()         # (10, 100)\n",
    "        freq = torch.from_numpy(self.top_amount[idx]).float()  # (10,)\n",
    "        pat = pat.unsqueeze(0)  # → (1, 10, 100)\n",
    "        return dna, pat, freq\n",
    "\n",
    "train_dataset = GenomicClipDataset(train_seq_h, train_top_out, train_top_amount)\n",
    "val_dataset   = GenomicClipDataset(val_seq_h,   val_top_out,   val_top_amount)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Define Improved CLIPModel (ViT + Projection Heads + BatchNorm + Dropout)\n",
    "# ==============================================================================\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dna_dim: int = 256,\n",
    "                 dna_depth: int = 2,\n",
    "                 dna_heads: int = 8,\n",
    "                 pat_dim: int = 512,\n",
    "                 pat_depth: int = 2,\n",
    "                 pat_heads: int = 8,\n",
    "                 latent_dim: int = 512,\n",
    "                 dropout: float = 0.4):\n",
    "        super(CLIPModel, self).__init__()\n",
    "\n",
    "        # --- DNA Encoder (ViT) ---\n",
    "        self.dna_encoder = ViT(\n",
    "            image_size   = (5, 32),\n",
    "            patch_size   = (5, 8),\n",
    "            num_classes  = dna_dim,\n",
    "            dim          = dna_dim,\n",
    "            depth        = dna_depth,\n",
    "            heads        = dna_heads,\n",
    "            mlp_dim      = dna_dim * 2,\n",
    "            channels     = 1,\n",
    "            dropout      = dropout,\n",
    "            emb_dropout  = dropout\n",
    "        )\n",
    "        self.dna_proj = nn.Sequential(\n",
    "            nn.Linear(dna_dim, latent_dim),\n",
    "            nn.LayerNorm(latent_dim),   # keep LayerNorm here\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "\n",
    "        # --- Pattern Encoder (ViT) ---\n",
    "        self.pat_encoder = ViT(\n",
    "            image_size   = (10, 100),\n",
    "            patch_size   = (2, 10),\n",
    "            num_classes  = pat_dim,\n",
    "            dim          = pat_dim,\n",
    "            depth        = pat_depth,\n",
    "            heads        = pat_heads,\n",
    "            mlp_dim      = pat_dim * 2,\n",
    "            channels     = 1,\n",
    "            dropout      = dropout,\n",
    "            emb_dropout  = dropout\n",
    "        )\n",
    "        # Replace LayerNorm on raw ViT output with BatchNorm1d\n",
    "        self.pat_feat_bn = nn.BatchNorm1d(pat_dim, eps=1e-5, momentum=0.1)\n",
    "\n",
    "        # --- Frequency branch normalization ---\n",
    "        # Replace LayerNorm(10) with BatchNorm1d(10)\n",
    "        self.freq_bn = nn.BatchNorm1d(10, eps=1e-5, momentum=0.1)\n",
    "\n",
    "        self.freq_proj_input = nn.Linear(10, pat_dim)\n",
    "        self.freq_mlp = nn.Sequential(\n",
    "            nn.Linear(pat_dim, pat_dim),\n",
    "            nn.LayerNorm(pat_dim),   # keep LayerNorm inside freq MLP\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(pat_dim, pat_dim),\n",
    "            nn.LayerNorm(pat_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "\n",
    "        # --- Combiner MLP: (pat_dim*2 → pat_dim) with BatchNorm after first linear\n",
    "        self.combiner_mlp = nn.Sequential(\n",
    "            nn.Linear(pat_dim * 2, pat_dim),\n",
    "            nn.BatchNorm1d(pat_dim, eps=1e-5, momentum=0.1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(pat_dim, pat_dim),\n",
    "            nn.LayerNorm(pat_dim),   # final LayerNorm\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "\n",
    "        # Final projection: (pat_dim → latent_dim)\n",
    "        self.pat_proj = nn.Sequential(\n",
    "            nn.Linear(pat_dim, latent_dim),\n",
    "            nn.LayerNorm(latent_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "\n",
    "        # Learnable temperature parameter (initialized to log(1/0.07))\n",
    "        self.logit_scale_param = nn.Parameter(torch.ones([]) * math.log(1/0.07))\n",
    "\n",
    "        # Xavier initialization for all Linear layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, dna_tensor, pat_tensor, freq_tensor):\n",
    "        \"\"\"\n",
    "        dna_tensor:  (B, 5, 32)\n",
    "        pat_tensor:  (B, 1, 10, 100)\n",
    "        freq_tensor: (B, 10) ← raw top_amount\n",
    "        Returns:\n",
    "            dna_latent: (B, latent_dim)\n",
    "            pat_latent: (B, latent_dim)\n",
    "            logit_scale: scalar > 0\n",
    "        \"\"\"\n",
    "        # --- DNA branch ---\n",
    "        dna_input   = dna_tensor.unsqueeze(1)          # (B, 1, 5, 32)\n",
    "        dna_feat    = self.dna_encoder(dna_input)      # (B, dna_dim)\n",
    "        dna_latent  = self.dna_proj(dna_feat)          # (B, latent_dim)\n",
    "\n",
    "        # --- Pattern branch ---\n",
    "        pat_feat       = self.pat_encoder(pat_tensor)  # (B, pat_dim)\n",
    "        # BatchNorm1d expects input shape (B, C), so no transpose needed\n",
    "        pat_feat_norm  = self.pat_feat_bn(pat_feat)    # (B, pat_dim)\n",
    "\n",
    "        # --- Frequency branch ---\n",
    "        freq_normed    = self.freq_bn(freq_tensor)     # (B, 10)\n",
    "        freq_proj      = self.freq_proj_input(freq_normed)  # (B, pat_dim)\n",
    "        freq_feat      = self.freq_mlp(freq_proj)            # (B, pat_dim)\n",
    "\n",
    "        # --- Combine pattern + frequency ---\n",
    "        combined   = torch.cat([pat_feat_norm, freq_feat], dim=1)  # (B, 2*pat_dim)\n",
    "        comb_feat  = self.combiner_mlp(combined)                   # (B, pat_dim)\n",
    "        pat_latent = self.pat_proj(comb_feat)                       # (B, latent_dim)\n",
    "\n",
    "        # Clamp logit_scale to avoid numerical explosion\n",
    "        logit_scale = torch.clamp(self.logit_scale_param, -5.0, 5.0)\n",
    "        logit_scale = logit_scale.exp()\n",
    "\n",
    "        return dna_latent, pat_latent, logit_scale\n",
    "\n",
    "\n",
    "# Instantiate model and move to device\n",
    "model = CLIPModel(\n",
    "    dna_dim    = 256, dna_depth    = 8, dna_heads    = 8,\n",
    "    pat_dim    = 512, pat_depth    = 8, pat_heads    = 8,\n",
    "    latent_dim = 32,\n",
    "    dropout    = dropout_prob\n",
    ").to(device)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. Contrastive Loss (InfoNCE with Label Smoothing)\n",
    "# ==============================================================================\n",
    "def contrastive_loss(dna_latent, pat_latent, logit_scale, smoothing: float = 0.1):\n",
    "    \"\"\"\n",
    "    dna_latent:   (B, D)\n",
    "    pat_latent:   (B, D)\n",
    "    logit_scale:  scalar\n",
    "    smoothing:    float in [0,1]\n",
    "    Returns:\n",
    "        average of image-to-text and text-to-image KLDiv-based losses.\n",
    "    \"\"\"\n",
    "    B, D = dna_latent.size()\n",
    "    eps = 1e-8\n",
    "\n",
    "    # normalize embeddings\n",
    "    dna_norm = dna_latent / (dna_latent.norm(dim=1, keepdim=True) + eps)\n",
    "    pat_norm = pat_latent / (pat_latent.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "    logits = logit_scale * torch.matmul(dna_norm, pat_norm.t())  # (B, B)\n",
    "    labels = torch.arange(B, device=logits.device)\n",
    "    n_classes = logits.size(1)\n",
    "\n",
    "    # create smoothed targets\n",
    "    with torch.no_grad():\n",
    "        smooth_target = torch.full((B, n_classes),\n",
    "                                   smoothing / (n_classes - 1),\n",
    "                                   device=logits.device)\n",
    "        smooth_target[torch.arange(B), labels] = 1.0 - smoothing\n",
    "\n",
    "    # KLDiv between log_softmax and smoothed target\n",
    "    loss_i2p = F.kl_div(F.log_softmax(logits, dim=1), smooth_target, reduction=\"batchmean\")\n",
    "    loss_p2i = F.kl_div(F.log_softmax(logits.t(), dim=1), smooth_target, reduction=\"batchmean\")\n",
    "    return (loss_i2p + loss_p2i) / 2.0\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. Training & Validation Loop with Optional AMP, Extended Warmup, Early Stopping\n",
    "# ==============================================================================\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "total_steps  = n_epochs * len(train_loader)\n",
    "warmup_steps = warmup_epochs * len(train_loader)\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / float(max(1, warmup_steps))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps)))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# Mixed-precision scaler (enabled only if USE_AMP=True)\n",
    "scaler = GradScaler(enabled=USE_AMP)\n",
    "\n",
    "# Print initial frequency statistics for debugging\n",
    "freq_all = torch.from_numpy(top_amount_dataset)\n",
    "print(\n",
    "    \"Initial freq stats:\",\n",
    "    f\"min={freq_all.min().item():.1e}, max={freq_all.max().item():.1e},\",\n",
    "    f\"mean={freq_all.mean().item():.1e}, std={freq_all.std().item():.1e}\"\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "no_improve    = 0\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    train_loss_accum = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{n_epochs} [Train]\", leave=False)\n",
    "\n",
    "    for step, (dna_batch, pat_batch, freq_batch) in enumerate(train_bar):\n",
    "        dna_batch = dna_batch.to(device)\n",
    "        pat_batch = pat_batch.to(device)\n",
    "        freq_batch= freq_batch.to(device)\n",
    "\n",
    "        # 1) Check inputs for NaN or Inf\n",
    "        if dna_batch.isnan().any() or dna_batch.isinf().any():\n",
    "            raise RuntimeError(\"dna_batch contains NaN or Inf\")\n",
    "        if pat_batch.isnan().any() or pat_batch.isinf().any():\n",
    "            raise RuntimeError(\"pat_batch contains NaN or Inf\")\n",
    "        if freq_batch.isnan().any() or freq_batch.isinf().any():\n",
    "            raise RuntimeError(\"freq_batch contains NaN or Inf\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 2) Forward pass (with or without AMP)\n",
    "        if USE_AMP:\n",
    "            with autocast():\n",
    "                dna_latent, pat_latent, logit_scale = model(dna_batch, pat_batch, freq_batch)\n",
    "                if dna_latent.isnan().any():\n",
    "                    raise RuntimeError(\"dna_latent contains NaN\")\n",
    "                if pat_latent.isnan().any():\n",
    "                    raise RuntimeError(\"pat_latent contains NaN\")\n",
    "                if logit_scale.isnan().any():\n",
    "                    raise RuntimeError(\"logit_scale contains NaN\")\n",
    "                loss = contrastive_loss(dna_latent, pat_latent, logit_scale, smoothing=0.1)\n",
    "        else:\n",
    "            dna_latent, pat_latent, logit_scale = model(dna_batch, pat_batch, freq_batch)\n",
    "            if dna_latent.isnan().any():\n",
    "                raise RuntimeError(\"dna_latent contains NaN\")\n",
    "            if pat_latent.isnan().any():\n",
    "                raise RuntimeError(\"pat_latent contains NaN\")\n",
    "            if logit_scale.isnan().any():\n",
    "                raise RuntimeError(\"logit_scale contains NaN\")\n",
    "            loss = contrastive_loss(dna_latent, pat_latent, logit_scale, smoothing=0.1)\n",
    "\n",
    "        # 3) Check loss for NaN\n",
    "        if loss.isnan().any():\n",
    "            raise RuntimeError(\"Loss is NaN. Inspect dna_latent, pat_latent, and logit_scale.\")\n",
    "\n",
    "        # 4) Backward pass with anomaly detection\n",
    "        if USE_AMP:\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "                scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "        else:\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "                loss.backward()\n",
    "\n",
    "        # 5) Gradient clipping and check for NaNs in gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and param.grad.isnan().any():\n",
    "                raise RuntimeError(f\"Gradient for {name} contains NaN\")\n",
    "\n",
    "        # 6) Optimizer step\n",
    "        if USE_AMP:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss_accum += loss.item()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        train_bar.set_postfix(train_loss=f\"{loss.item():.4f}\", lr=f\"{current_lr:.2e}\")\n",
    "    train_bar.close()\n",
    "\n",
    "    avg_train_loss = train_loss_accum / len(train_loader)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    val_loss_accum = 0.0\n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch}/{n_epochs} [Val]\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for dna_batch, pat_batch, freq_batch in val_bar:\n",
    "            dna_batch = dna_batch.to(device)\n",
    "            pat_batch = pat_batch.to(device)\n",
    "            freq_batch= freq_batch.to(device)\n",
    "\n",
    "            if USE_AMP:\n",
    "                with autocast():\n",
    "                    dna_latent, pat_latent, logit_scale = model(dna_batch, pat_batch, freq_batch)\n",
    "                    loss = contrastive_loss(dna_latent, pat_latent, logit_scale, smoothing=0.1)\n",
    "            else:\n",
    "                dna_latent, pat_latent, logit_scale = model(dna_batch, pat_batch, freq_batch)\n",
    "                loss = contrastive_loss(dna_latent, pat_latent, logit_scale, smoothing=0.1)\n",
    "\n",
    "            val_loss_accum += loss.item()\n",
    "            val_bar.set_postfix(val_loss=f\"{loss.item():.4f}\")\n",
    "    val_bar.close()\n",
    "\n",
    "    avg_val_loss = val_loss_accum / len(val_loader)\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{n_epochs} — \"\n",
    "        f\"Avg Train Loss: {avg_train_loss:.4f} | \"\n",
    "        f\"Avg Val Loss:   {avg_val_loss:.4f} | \"\n",
    "        f\"LR: {scheduler.get_last_lr()[0]:.2e}\"\n",
    "    )\n",
    "\n",
    "    # Early Stopping & Checkpoint\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        no_improve    = 0\n",
    "        ckpt_path     = os.path.join(checkpoint_dir, \"best_clip_model.pth\")\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"scheduler_state\": scheduler.state_dict(),\n",
    "            \"best_val_loss\": best_val_loss\n",
    "        }, ckpt_path)\n",
    "        print(f\"  ► Saved new best checkpoint (Val Loss: {best_val_loss:.4f})\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"No improvement for {patience} epochs. Stopping early at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. Save Final Model Checkpoint (if not already saved in early stop)\n",
    "# ==============================================================================\n",
    "final_ckpt = os.path.join(checkpoint_dir, \"last_epoch_clip_model.pth\")\n",
    "torch.save({\n",
    "    \"epoch\": epoch,\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"optimizer_state\": optimizer.state_dict(),\n",
    "    \"scheduler_state\": scheduler.state_dict(),\n",
    "    \"best_val_loss\": best_val_loss\n",
    "}, final_ckpt)\n",
    "print(f\"Training complete. Final checkpoint saved to {final_ckpt}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
